\chapter{Related work}
The internet traffic explosion started in the mid-90s induced the creation of massive datasets. Processing very large amount of data, even though can be conceptually straightforward, actually means many computations that have to be distributed across hundreds or thousands of machines in order to finish in a reasonable amount of time. In early 2000s, the developer had the burden to write complex applications responsible for parallelizing the computation, distributing the data, and handling possible failures. As a reaction to this complexity, Google in 2004 published a paper describing a novel programming model for processing large datasets named MapReduce \cite{googlemapreduce}. MapReduce provides an abstraction that allows the developer to focus on the computation he/she wants to express hiding the details of parallelization, fault-tolerance, data distribution and load balancing in a framework. The MapReduce framework takes advantage of the Google File System \cite{googlefilesystem}: a distributed file system that uses replication to provide availability and reliability on top of commodity unreliable hardware. The open source community, inspired by the Google MapReduce programming model, created Hadoop \cite{hadooponline} and HDFS \cite{hadoophdfs}. The Hadoop release boosted the growth of the big data analysis field and created new scenarios and new type of applications that were very tough to realize with previous existing tools. From that moment on, a lot of effort has been put in realizing more efficient and high-performance technologies for collecting, aggregating and analyzing huge amount of data.

In this chapter we will discuss some of the technologies and architectures that can be used in order to process large amount of data generated by (IoT) sensors, possibly in real-time.

\section{Batch-Mode architecture}
Google MapReduce and Hadoop are designed to be executed on top of clusters of computing nodes. The first releases of Hadoop integrated the cluster resource management with the data processing engine itself (Hadoop MapReduce). In 2012, Hadoop 2.0 was released \cite{hadooponline} and it introduced YARN \cite{yarn}; Apache Yet Another Resource Negotiator (YARN) decouples the programming model from the resource management infrastructure and the users' applications scheduling. In general, we can think to YARN as a distributed operating system that is capable of managing a cluster of machines and scheduling users' applications on the available resources. Furthermore, it provides an API that can be used to develop any generic distributed application. Hence, different computing frameworks that address different application requirements have been built on top of YARN (e.g. the general purpose Apache Spark \cite{apachesparkonline}, Apache Giraph \cite{apachegiraphonline} for iterative graph processing, etc.). 

 \begin{figure}[ht!]
\includegraphics[width=1\textwidth]{images/barch_architecture.pdf}
 \caption{Classic big data batch architecture}
\label{fig:batch_architecture}
\end{figure}

A big data application usually has a well defined architecture, no matter the computing framework it uses and whether or not it runs on top of YARN or on another cluster management system (e.g. Apache Mesos  \cite{apachemesosonline}, Kubernetes \cite{kubernetesonline}, in the cloud using Amazon Elastic Map Reduce (EMR) \cite{amazonemronline}). The \emph{classic} big data architecture for processing large amount of structured or unstructured data is named \emph{batch architecture} \cite{philosophydistributeddata, fastdataarchitecture, streamingbigdataprocessinginclouds}. The typical big data batch architecture consists of the following components or layers (see Figure \ref{fig:batch_architecture}):
\begin{itemize}
\item	 \emph{Data ingestion layer}. The data ingestion layer can be used to move data that can come from different sources into the persistence tier. The data ingestion layer can take advantage of special-purpose services such as Apache Flume \cite{apacheflumeonline} for log aggregation or Apache Sqoop \cite{apachesqooponline} for interoperating with databases.
\item \emph{Data persistence.} The data persistence layer is responsible of storing and indexing final as well intermediate datasets. This tier can store large amount of data using one of the next-generation database systems. In fact, the Dynamo \cite{dynamo} paper accelerated interest in NoSQL systems and many NoSQL database emerged such as Apache Cassandra \cite{apachecassandraonline} or MongoDB \cite{mongodbonline}. Furthermore, the CAP theorem emerged as a way of understanding the trade-offs between consistency and availability of service in distributed systems when a network partition occurs. For the always-on applications, it often made sense to accept \emph{eventual consistency} in exchange for a greater availability \cite{fastdataarchitecture}. In big data application, database systems are often used when row-level access (e.g. CRUD operations) is required \cite{fastdataarchitecture}.

In order to store large datasets, the data persistence layer can also use a distributed file system or object store, such as HDFS or Amazon S3 \cite{awss3online}. These systems offers lower cost per GB storage compared to databases and more flexibility for data formats \cite{fastdataarchitecture}. However, they are best used when scans are the dominant access pattern, rather than CRUD operations. 
\item  \emph{Cluster resource management}. 
\item  \emph{Data Processing.} The data analysis jobs written in Hadoop MapReduce, Spark, or other tools submitted to the cluster resource manager.
\end{itemize}

A typical big data application can be built as a \emph{workflow}, a directed graph of distributed computing jobs \cite{bigdataatlinkedin}. Each job reads one or more input datasets from the persistent storage and produces one or more output datasets. In MapReduce, if the application requires data reuse between computations (e.g. between two jobs) the intermediate result is also written to an external stable storage system. This can incur substantial overheads due to data replication, disk I/O, and serialization, which can dominate application execution times. Recognizing this problem, researchers have developed specialized frameworks for some applications that require data reuse. These frameworks only support specific computation patterns and perform data sharing implicitly for these patterns. Eventually, the general purpose computing framework Apache Spark \cite{apachespark} was released. Spark is the first system that allows a general-purpose programming language to be used at interactive speeds for in-memory data mining on clusters \cite{apachespark}. Spark is based on the concept of resilient distributed datasets (RDDs) \cite{apachesparkrdd} that enables efficient data reuse in a broad range of applications. The RDD abstraction allows users to explicitly persist intermediate results in memory, control the data partitioning to optimize the data placement, and manipulate them using a rich set of operators. Even though Spark increased the interactivity of many big data applications and tools, a typical job in batch-mode can last from few minutes to several hours. In fact, in classic batch-mode, data is collected and stored in the persistent storage layer and once in a while (e.g. every hour, once per day) the data is processed through one or more batch jobs. The high-latency imposed by the batch architecture is not suitable for real-time applications in which data should be computed as it is generated, in an incremental fashion.

\section{Real-time streaming architecture}
Recently, many big data real-time applications emerged such as detecting fraudulent financial activity as it happens or optimizing e-commerce search results in real-time. These kinds of applications process unbounded datasets and potentially infinite \emph{streams} of data, and they are often  referred as real-time streaming applications. The backbone of a real-time streaming application is usually a distributed, high-performance and always available architecture. There are many streaming systems and ways of doing streaming, and everything is evolving quickly. In this section, we will discuss and highlight the components required to build an efficient streaming architecture.



